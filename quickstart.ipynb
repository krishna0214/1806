{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TurboML Quickstart"
      ],
      "id": "0",
      "metadata": {
        "id": "0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the environment and install TurboML's SDK.\n",
        "We use `turboml-installer` to set up the environment for TurboML's SDK."
      ],
      "metadata": {
        "id": "M8jNR5T8SqqW"
      },
      "id": "M8jNR5T8SqqW"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q turboml-installer\n",
        "import turboml_installer ; turboml_installer.install_on_colab()"
      ],
      "execution_count": null,
      "metadata": {
        "id": "UpD-JPscSqqX"
      },
      "outputs": [],
      "id": "UpD-JPscSqqX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel should now be restarted with TurboML's SDK installed."
      ],
      "metadata": {
        "id": "BNhvOLJ8SqqZ"
      },
      "id": "BNhvOLJ8SqqZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to your TurboML instance\n",
        "\n",
        "Note that you can copy and replace this snippet with one from your TurboML homepage."
      ],
      "metadata": {
        "id": "HSq2rPAESqqb"
      },
      "id": "HSq2rPAESqqb"
    },
    {
      "cell_type": "code",
      "source": [
        "import turboml as tb\n",
        "tb.init(backend_url=BACKEND_URL, api_key=API_KEY)"
      ],
      "execution_count": null,
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Data\n",
        "\n",
        "TurboML is built for real-time machine learning, and as such, deals with streams of data. This can be achieved by using connectors to continuously pull data from your data source (like S3  or postgres), or use push-based approaches using REST API or Client SDKs.\n",
        "\n",
        "For the purpose of this tutorial, we can use simulate real-time data generation, with a batch-like setting using pandas dataframes. Let's first load some pandas dataframes. In this example, we're using a credit card fraud detection dataset."
      ],
      "id": "2",
      "metadata": {
        "id": "2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df = tb.datasets.FraudDetectionDatasetFeatures().df\n",
        "labels_df = tb.datasets.FraudDetectionDatasetLabels().df"
      ],
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df"
      ],
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df"
      ],
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset has 201406 datapoints, each with a corresponding label. Since we don't have a natural primary key in the dataset that can uniquely identify each row, we'll use the inbuilt index that pandas provides."
      ],
      "id": "6",
      "metadata": {
        "id": "6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df.head()"
      ],
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df.head()"
      ],
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion"
      ],
      "id": "9",
      "metadata": {
        "id": "9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now upload these dataframes to the TurboML platform, the **OnlineDataset** class can be used here. It takes in the dataframe, the primary key,  and the name of the dataset that is to be created for the given dataframe as input."
      ],
      "id": "10",
      "metadata": {
        "id": "10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to create and upload dataset\n",
        "transactions = tb.OnlineDataset.from_pd(\n",
        "    id=\"qs_transactions\",\n",
        "    df=transactions_df,\n",
        "    key_field=\"transactionID\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "labels = tb.OnlineDataset.from_pd(\n",
        "    id=\"qs_transaction_labels\",\n",
        "    df=labels_df,\n",
        "    key_field=\"transactionID\",\n",
        "    load_if_exists=True,\n",
        ")"
      ],
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "TurboML platform facilitates transformations on raw data to produce new features. You can use the jupyter notebook as a \"playground\" to explore different features. This involves 3 steps.\n",
        "- **fetch data**: Experimentation is easier on static data. Since TurboML works with continuous data streams, to enable experimentation we fetch a snapshot or a subset of data in the jupyter notebook.\n",
        "- **add feature definitions**: Now that we have a static dataset, we can define multiple different features, and see their values on this dataset. Since we can observe their values, we can perform simple experiments and validations like correlations, plots and other exploratory analysis.\n",
        "- **submit feature definitions**: Once we're confident about the features we've defined, we can now submit the ones we want TurboML to compute continuously for the actual data stream."
      ],
      "id": "12",
      "metadata": {
        "id": "12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch data\n",
        "\n",
        "We can use the **get_features** function to get a snapshot or subset of the data stream.\n",
        "\n",
        "**Note**: This size of the dataset returned by this function can change on each invocation. Also, the dataset is not guaranteed to be in the same order."
      ],
      "id": "13",
      "metadata": {
        "id": "13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add feature definitions\n",
        "\n",
        "To add feature definitions, we have a class from turboml package called **FeatureEngineering**. This allows us to define SQL-based and dynamic aggregation-based features."
      ],
      "id": "14",
      "metadata": {
        "id": "14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows how to define an SQL-based feature. The sql_definition parameter in the **create_sql_features** function takes in the SQL expression to be used to prepare the feature. It returns a dataframe with all the original columns, and another column which, on a high-level is defined as ```SELECT sql_definition AS new_feature_name FROM dataframe```."
      ],
      "id": "15",
      "metadata": {
        "id": "15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.feature_engineering.create_sql_features(\n",
        "    sql_definition='\"transactionAmount\" + \"localHour\"',\n",
        "    new_feature_name=\"my_sql_feat\",\n",
        ")"
      ],
      "execution_count": null,
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.feature_engineering.get_local_features()"
      ],
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb.get_timestamp_formats()"
      ],
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.feature_engineering.register_timestamp(\n",
        "    column_name=\"timestamp\", format_type=\"epoch_seconds\"\n",
        ")"
      ],
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows how to define an aggregation-based feature using the **create_aggregate_features** function. It returns a dataframe with all the original columns, and another column which, on a high-level is defined as ```SELECT operation(column_to_operate) OVER (PARTITION BY column_to_group ORDER BY time_column RANGE BETWEEN INTERVAL window_duration PRECEDING AND CURRENT ROW) as new_feature_name from dataframe```."
      ],
      "id": "20",
      "metadata": {
        "id": "20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.feature_engineering.create_aggregate_features(\n",
        "    column_to_operate=\"transactionAmount\",\n",
        "    column_to_group=\"accountID\",\n",
        "    operation=\"SUM\",\n",
        "    new_feature_name=\"my_sum_feat\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    window_duration=24,\n",
        "    window_unit=\"hours\",\n",
        ")"
      ],
      "execution_count": null,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.feature_engineering.get_local_features()"
      ],
      "execution_count": null,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit feature definitions\n",
        "\n",
        "Now that we've seen the newly created features, and everything looks good, we can submit these feature definitions to the TurboML platform so that this can be computed continously for the input data stream."
      ],
      "id": "23",
      "metadata": {
        "id": "23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to tell the platform to start computations for all pending features for the given dataset. This can be done by calling the **materialize_features** function."
      ],
      "id": "24",
      "metadata": {
        "id": "24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.feature_engineering.materialize_features([\"my_sql_feat\", \"my_sum_feat\"])"
      ],
      "execution_count": null,
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transactions = transactions.feature_engineering.get_materialized_features()\n",
        "df_transactions"
      ],
      "execution_count": null,
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Modelling\n",
        "\n",
        "TurboML provides out of the box algorithms, optimized for real-time ML, and supports bringing your own models and algorithms as well. In this tutorial, we'll use the algorithms provided by TurboML."
      ],
      "id": "27",
      "metadata": {
        "id": "27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the available algorithms\n",
        "\n",
        "You can check what are the available ML algorithms based on `tb.ml_algorithms(have_labels=True/False)` depending on supervised or unsupervised learning."
      ],
      "id": "28",
      "metadata": {
        "id": "28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.ml_algorithms(have_labels=False)"
      ],
      "execution_count": null,
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the RandomCutForest (RCF) algorithm."
      ],
      "id": "30",
      "metadata": {
        "id": "30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create model\n",
        "\n",
        "Now that we've chosen an algorithm, we need to create a model."
      ],
      "id": "31",
      "metadata": {
        "id": "31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tb.RCF(number_of_trees=50)"
      ],
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Streaming ML jobs\n",
        "Now that we've instantiated the model, we can deploy it using the **deploy** function.\n",
        "For an unsupervised ML job, we need to provide a dataset from which the model can consume inputs. For each record in this dataset, the model will make a prediction, produce the prediction to an output dataset, and then perform unsupervised updates using this record.\n",
        "\n",
        "There are four types of fields that can be used by any ML algorithm:\n",
        "\n",
        "  - numerical_fields: This represents fields that we want our algorithm to treat as real-valued fields.\n",
        "  - categorical_fields: This represents fields that we want our algorithm to treat as categorical fields.\n",
        "  - time_field: This is used for time-series applications to capture the timestamp field.\n",
        "  - textual_fields: This represents fields that we want our algorithm to treat as text fields.\n",
        "\n",
        "The input values from any of these fields are suitably converted to the desired type. String values are converted using the hashing trick."
      ],
      "id": "33",
      "metadata": {
        "id": "33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's construct a model config using the following numerical fields, no categorical or time fields."
      ],
      "id": "34",
      "metadata": {
        "id": "34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_fields = [\n",
        "    \"transactionAmount\",\n",
        "    \"localHour\",\n",
        "    \"my_sum_feat\",\n",
        "    \"my_sql_feat\",\n",
        "]\n",
        "features = transactions.get_model_inputs(numerical_fields=numerical_fields)\n",
        "label = labels.get_model_labels(label_field=\"is_fraud\")"
      ],
      "execution_count": null,
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_model_rcf = model.deploy(name=\"demo_model_rcf\", input=features, labels=label)"
      ],
      "execution_count": null,
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect model outputs\n",
        "\n",
        "We can now fetch the outputs that the model produced by calling the **get_outputs** function.\n",
        "\n",
        "**Note**: This size of the outputs returned by this function can change on each invocation, since the model is continuosly producing outputs.  "
      ],
      "id": "37",
      "metadata": {
        "id": "37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = deployed_model_rcf.get_outputs()"
      ],
      "execution_count": null,
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(outputs)"
      ],
      "execution_count": null,
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output = outputs[-1]\n",
        "sample_output"
      ],
      "execution_count": null,
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output corresponds to an input with the key, or index, sample_output.key. Along with the anomaly score, the output also contains attributions to different features. We can see that the first numerical feature, i.e. 'transactionAmount' is around sample_output.feature_score[0]*100% responsible for the anomaly score"
      ],
      "id": "41",
      "metadata": {
        "id": "41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([output[\"record\"].score for output in outputs])"
      ],
      "execution_count": null,
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Endpoints\n",
        "\n",
        "The above method of interacting with the model was asynchronous. We were adding our datapoints to an input dataset, and getting the corresponding model outputs in an output dataset. In some scenarios, we need a synchronous method to query the model. This is where we can use the model endpoints that TurboML exposes."
      ],
      "id": "43",
      "metadata": {
        "id": "43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_endpoints = deployed_model_rcf.get_endpoints()\n",
        "model_endpoints"
      ],
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know what endpoint to send the request to, we now need to figure out the right format. Let's try to make a prediction on the last row from our input dataset."
      ],
      "id": "45",
      "metadata": {
        "id": "45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_query_datapoint = transactions_df.iloc[-1].to_dict()\n",
        "model_query_datapoint"
      ],
      "execution_count": null,
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.post(\n",
        "    model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n",
        ")"
      ],
      "execution_count": null,
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp.json()"
      ],
      "execution_count": null,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Inference on Models\n",
        "\n",
        "While the above method is more suited for individual requests, we can also perform batch inference on the models. We use the **get_inference** function for this purpose."
      ],
      "id": "49",
      "metadata": {
        "id": "49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = deployed_model_rcf.get_inference(transactions_df)\n",
        "outputs"
      ],
      "execution_count": null,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "id": "51",
      "metadata": {
        "id": "51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to ML models, TurboML provides in-built metrics, and supports defining your own metrics. Let's see the available metrics."
      ],
      "id": "52",
      "metadata": {
        "id": "52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.evaluation_metrics()"
      ],
      "execution_count": null,
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can select the AreaUnderCurve (AUC) metric to evaluate our anomaly detection model. The windowed prefix means we're evaluating these metrics over a rolling window. By default, the window size is `1000`."
      ],
      "id": "54",
      "metadata": {
        "id": "54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_model_rcf.add_metric(\"WindowedAUC\")"
      ],
      "execution_count": null,
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to steps like feature engineering and ML modelling, model evaluation is also a continuosly running job. We can look at the snapshot of the model metrics at any given instance by using the **get_evaluation** function.\n",
        "\n",
        "**Note**: This size of the outputs returned by this function can change on each invocation, since we're continuously evaluating the model.  "
      ],
      "id": "56",
      "metadata": {
        "id": "56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_auc_scores = deployed_model_rcf.get_evaluation(\"WindowedAUC\")\n",
        "model_auc_scores[-1]"
      ],
      "execution_count": null,
      "id": "57",
      "metadata": {
        "id": "57"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
      ],
      "execution_count": null,
      "id": "58",
      "metadata": {
        "id": "58"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation with filter and custom window size"
      ],
      "id": "59",
      "metadata": {
        "id": "59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We support running evaluation on filtered model data using valid SQL expression along with custom window size."
      ],
      "id": "60",
      "metadata": {
        "id": "60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_auc_scores = deployed_model_rcf.get_evaluation(\n",
        "    \"WindowedAUC\",\n",
        "    filter_expression=\"input_data.transactionCurrencyCode != 'USD' AND output_data.score > 0.6\",\n",
        "    window_size=200,\n",
        ")\n",
        "model_auc_scores[-1]"
      ],
      "execution_count": null,
      "id": "61",
      "metadata": {
        "id": "61"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
      ],
      "execution_count": null,
      "id": "62",
      "metadata": {
        "id": "62"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning\n",
        "\n",
        "Let's now take an example with a supervised learning algorithm. First, let's see what algorithms are supported out of the box."
      ],
      "id": "63",
      "metadata": {
        "id": "63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.ml_algorithms(have_labels=True)"
      ],
      "execution_count": null,
      "id": "64",
      "metadata": {
        "id": "64"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use HoeffdingTreeClassifier to try to classify fraudulent and normal activity on the same dataset. First, we need to instantiate a model."
      ],
      "id": "65",
      "metadata": {
        "id": "65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "htc_model = tb.HoeffdingTreeClassifier(n_classes=2)"
      ],
      "execution_count": null,
      "id": "66",
      "metadata": {
        "id": "66"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same numerical fields in this model as well. However, let's add some categorical fields as well."
      ],
      "id": "67",
      "metadata": {
        "id": "67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_fields = [\n",
        "    \"digitalItemCount\",\n",
        "    \"physicalItemCount\",\n",
        "    \"isProxyIP\",\n",
        "]\n",
        "features = transactions.get_model_inputs(\n",
        "    numerical_fields=numerical_fields, categorical_fields=categorical_fields\n",
        ")\n",
        "label = labels.get_model_labels(label_field=\"is_fraud\")"
      ],
      "execution_count": null,
      "id": "68",
      "metadata": {
        "id": "68"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Supervised ML jobs\n",
        "Same as before, we can deploy this model with the **deploy** function."
      ],
      "id": "69",
      "metadata": {
        "id": "69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_model_htc = htc_model.deploy(\"demo_classifier\", input=features, labels=label)"
      ],
      "execution_count": null,
      "id": "70",
      "metadata": {
        "id": "70"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now inspect the outputs."
      ],
      "id": "71",
      "metadata": {
        "id": "71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = deployed_model_htc.get_outputs()"
      ],
      "execution_count": null,
      "id": "72",
      "metadata": {
        "id": "72"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(outputs)"
      ],
      "execution_count": null,
      "id": "73",
      "metadata": {
        "id": "73"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output = outputs[-1]\n",
        "sample_output"
      ],
      "execution_count": null,
      "id": "74",
      "metadata": {
        "id": "74"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that since this is a classification model, we have some new attributes in the output, specifically `class_probabilities` and `predicted_class`. We also have the `score` attribute which, for classification, just shows us the probability for the last class."
      ],
      "id": "75",
      "metadata": {
        "id": "75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Model Endpoints\n",
        "Predict API for supervised models is exactly the same as unsupervised models."
      ],
      "id": "76",
      "metadata": {
        "id": "76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_endpoints = deployed_model_htc.get_endpoints()\n",
        "model_endpoints"
      ],
      "execution_count": null,
      "id": "77",
      "metadata": {
        "id": "77"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.post(\n",
        "    model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n",
        ")\n",
        "resp.json()"
      ],
      "execution_count": null,
      "id": "78",
      "metadata": {
        "id": "78"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Model Evaluation\n",
        "Let's now evaluate our supervised ML model. The process is exactly the same as for unsupervised model evaluation."
      ],
      "id": "79",
      "metadata": {
        "id": "79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_model_htc.add_metric(\"WindowedAUC\")"
      ],
      "execution_count": null,
      "id": "80",
      "metadata": {
        "id": "80"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same **get_evaluation** function to fetch the metrics for this model as well. Remember, this function retrieves the metric values present at that moment of time. So, if the number of records recieved seem low, just re-run this function.  "
      ],
      "id": "81",
      "metadata": {
        "id": "81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\n",
        "model_auc_scores[-1]"
      ],
      "execution_count": null,
      "id": "82",
      "metadata": {
        "id": "82"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
      ],
      "execution_count": null,
      "id": "83",
      "metadata": {
        "id": "83"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison\n",
        "\n",
        "Now that we have 2 models deployed, and we've registered metrics for both of them, we can compare them on real-time data. On each invocation, the following function will fetch the latest evaluations of the models and plot them."
      ],
      "id": "84",
      "metadata": {
        "id": "84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.compare_model_metrics(\n",
        "    models=[deployed_model_rcf, deployed_model_htc], metric=\"WindowedAUC\"\n",
        ")"
      ],
      "execution_count": null,
      "id": "85",
      "metadata": {
        "id": "85"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deletion\n",
        "\n",
        "We can delete the models like this, by default the generated output is deleted. If you want to retain the output generated by model, use `delete_output_topic=False`."
      ],
      "id": "86",
      "metadata": {
        "id": "86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_model_rcf.delete()"
      ],
      "execution_count": null,
      "id": "87",
      "metadata": {
        "id": "87"
      },
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}